{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b905b2d3",
   "metadata": {},
   "source": [
    "# This notebook experiments on various pruning techniques adapted and modified from various papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5418242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "keras-unet init: TF version is >= 2.0.0 - using `tf.keras` instead of `Keras`\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import tempfile\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow import keras\n",
    "from keras_unet_collection import models\n",
    "import keras_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b49e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define custom objects\n",
    "def jaccard_distance(y_true, y_pred, smooth=100):\n",
    "        intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "        sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "        jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "        return (1 - jac) * smooth\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    smooth = 1.0\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (\n",
    "                K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(self, y_true, y_pred):\n",
    "    loss = 1 - self._dice_coef(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "class GELU(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GELU, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return keras.activations.gelu(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc107e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(r\"C:\\Users\\UAB\\Segmentation - Main1_CK\\Human Model\\Keras\\runet_kid_best_train.h5\", custom_objects={\n",
    "                       'jaccard_distance': jaccard_distance,\n",
    "                       'dice_coef_loss': dice_coef_loss,\n",
    "                       'dice_coef': dice_coef}) #load the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282d986e",
   "metadata": {},
   "source": [
    "## Weight Pruning\n",
    "Weight pruning is a technique used to reduce the size of neural network models by identifying and removing the least important weights. The code for weight pruning involves calculating the importance scores of weights based on certain criteria and applying a threshold to selectively remove the least important weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e482e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import clone_model\n",
    "\n",
    "class WeightPruning:\n",
    "    def __init__(self, trained_model):\n",
    "        self.trained_model = trained_model\n",
    "        self.all_weights_sorted = None  # Placeholder for sorted weights\n",
    "        self.total_no_weights = None  # Placeholder for the total number of weights\n",
    "        self.total_no_layers = len(trained_model.layers)  # Total number of layers in the model\n",
    "        self.pruning_percentages = None  # Placeholder for pruning percentages\n",
    "\n",
    "    def sort_all_weights(self):\n",
    "        all_weights = {}  # Dictionary to store weights of each layer\n",
    "        for layer_no in range(self.total_no_layers):\n",
    "            layer_weights = self.trained_model.layers[layer_no].get_weights()  # Get weights of the layer\n",
    "            if layer_weights:\n",
    "                layer_weights_flat = np.abs(layer_weights[0].flatten())  # Flatten and take absolute values of the weights\n",
    "                all_weights[layer_no] = layer_weights_flat  # Store the flattened weights in the dictionary\n",
    "\n",
    "        # Sort the weights based on the maximum value in each layer\n",
    "        self.all_weights_sorted = {k: v for k, v in sorted(all_weights.items(), key=lambda item: np.max(item[1]))}\n",
    "        self.total_no_weights = sum(len(w) for w in self.all_weights_sorted.values())  # Calculate the total number of weights\n",
    "\n",
    "    def prune_weights(self, pruning_percent):\n",
    "        self.sort_all_weights()\n",
    "        self.pruning_percentages = pruning_percent\n",
    "\n",
    "        pruned_model = clone_model(self.trained_model)  # Create a copy of the trained model\n",
    "        pruned_model.build((None,) + self.trained_model.input_shape[1:])  # Build the pruned model with the same input shape\n",
    "        pruned_model.set_weights(self.trained_model.get_weights())  # Set the initial weights of the pruned model\n",
    "\n",
    "        prune_fraction = self.pruning_percentages / 100  # Convert pruning percentage to fraction\n",
    "        num_weights_to_prune = int(prune_fraction * self.total_no_weights)  # Calculate the number of weights to prune\n",
    "        pruned_weights = {k: v for k, v in list(self.all_weights_sorted.items())[:num_weights_to_prune]}  # Select the weights to prune\n",
    "\n",
    "        for layer_no, weights_flat in pruned_weights.items():\n",
    "            weights_shape = self.trained_model.layers[layer_no].get_weights()[0].shape  # Get the shape of the weights in the original model\n",
    "            pruned_weights_reshaped = np.reshape(weights_flat, weights_shape)  # Reshape the pruned weights to match the original shape\n",
    "            pruned_layer_weights = [pruned_weights_reshaped] + self.trained_model.layers[layer_no].get_weights()[1:]  # Combine the pruned weights with the remaining weights of the layer\n",
    "            pruned_model.layers[layer_no].set_weights(pruned_layer_weights)  # Set the pruned weights in the corresponding layer of the pruned model\n",
    "\n",
    "        return pruned_model  # Return the pruned model\n",
    "\n",
    "    def get_total_parameters(self):\n",
    "        total_params = self.trained_model.count_params()  # Calculate the total number of parameters in the trained model\n",
    "        return total_params\n",
    "\n",
    "    def get_pruned_parameters(self, pruning_percent):\n",
    "        self.sort_all_weights()\n",
    "        prune_fraction = pruning_percent / 100  # Convert pruning percentage to fraction\n",
    "        num_weights_to_prune = int(prune_fraction * self.total_no_weights)  # Calculate the number of weights to prune\n",
    "        pruned_params = self.total_no_weights - num_weights_to_prune  # Calculate the number of remaining parameters after pruning\n",
    "        return pruned_params  # Return the number of pruned parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a91d6048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Parameters: 7872194\n",
      "Pruned Model Parameters: 3930432\n"
     ]
    }
   ],
   "source": [
    "pruning_percent = 50  # Example pruning percentage\n",
    "pruner = WeightPruning(model)\n",
    "pruned_model = pruner.prune_weights(pruning_percent)\n",
    "pruned_params = pruner.get_pruned_parameters(pruning_percent)\n",
    "total_params = pruner.get_total_parameters()\n",
    "\n",
    "print(\"Original Model Parameters:\", total_params)\n",
    "print(\"Pruned Model Parameters:\", pruned_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f509d871",
   "metadata": {},
   "source": [
    "## Unit Pruning\n",
    "Unit pruning is a technique used to reduce the size of neural network models by identifying and removing the least important units (neurons or filters). The code for unit pruning involves calculating the importance scores of units based on certain criteria and selectively removing the units with the lowest scores to reduce model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65ce64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D\n",
    "import numpy as np\n",
    "\n",
    "class UnitPruning:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.original_params = self.count_model_params()  # Calculate the original number of parameters in the model\n",
    "\n",
    "    def count_model_params(self):\n",
    "        # Count the number of parameters in Conv2D layers of the model\n",
    "        return np.sum([np.prod(layer.get_weights()[0].shape) for layer in self.model.layers if isinstance(layer, Conv2D)])\n",
    "\n",
    "    def prune(self, pruning_threshold):\n",
    "        pruned_params = 0  # Counter for the pruned parameters\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, Conv2D):  # Check if the layer is a Conv2D layer\n",
    "                weights = layer.get_weights()  # Get the weights of the layer\n",
    "                mask = np.abs(weights[0]) > pruning_threshold  # Create a mask based on the pruning threshold\n",
    "                pruned_params += np.sum(mask == False)  # Count the number of pruned parameters\n",
    "                weights[0] = weights[0] * mask.astype(np.float32)  # Apply the mask to the weights\n",
    "                layer.set_weights(weights)  # Set the pruned weights in the layer\n",
    "\n",
    "        pruned_params_percent = (pruned_params / self.original_params) * 100  # Calculate the percentage of pruned parameters\n",
    "        print(\"Original Model Parameters:\", self.original_params)  # Print the original number of parameters\n",
    "        print(\"Pruned Model Parameters:\", self.original_params - pruned_params)  # Print the number of parameters after pruning\n",
    "        print(\"Pruned Parameters Percentage:\", pruned_params_percent)  # Print the percentage of pruned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a131f4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Parameters: 7857216\n",
      "Pruned Model Parameters: 20511\n",
      "Pruned Parameters Percentage: 99.738953339198\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the UnitPruning class\n",
    "pruner = UnitPruning(model)\n",
    "\n",
    "# Prune the model with a given pruning threshold\n",
    "pruning_threshold = 0.1\n",
    "pruner.prune(pruning_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b5473",
   "metadata": {},
   "source": [
    "## Structured Pruning\n",
    "#### Use this code if you want to zero the weights and not remove them, maintaing the number of parameters\n",
    "Structured pruning is a technique used to reduce the size of neural network models by removing entire structured components such as layers, channels, or filters, instead of individual weights or units. The code for structured pruning involves identifying and removing the least important structured components based on specific criteria, resulting in a more compact model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ade4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class StructuredPruning(Layer):\n",
    "    def __init__(self, pruning_percentage, **kwargs):\n",
    "        super(StructuredPruning, self).__init__(**kwargs)\n",
    "        self.pruning_percentage = pruning_percentage\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a pruning mask as a trainable weight\n",
    "        self.pruning_mask = self.add_weight(name='pruning_mask', shape=input_shape[1:], initializer='ones', trainable=False)\n",
    "        super(StructuredPruning, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply pruning mask to the inputs\n",
    "        pruned_weights = K.cast(K.greater_equal(self.pruning_mask, 1.0), K.floatx())\n",
    "        pruned_inputs = inputs * pruned_weights\n",
    "        return pruned_inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(StructuredPruning, self).get_config()\n",
    "        config.update({'pruning_percentage': self.pruning_percentage})\n",
    "        return config\n",
    "\n",
    "def apply_structured_pruning(model, pruning_percentage):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, StructuredPruning):\n",
    "            # Apply structured pruning to the layer by scaling the pruning mask\n",
    "            layer.set_weights([layer.get_weights()[0] * (layer.pruning_percentage / 100.0)])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9afcb37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_structured_pruning(model, pruning_percentage):\n",
    "    pruned_model = model\n",
    "    for layer in pruned_model.layers:\n",
    "        if isinstance(layer, StructuredPruning):\n",
    "            weights = layer.get_weights()[0]\n",
    "            mask = K.cast(K.greater_equal(K.abs(weights), K.percentile(K.abs(weights), pruning_percentage)), K.floatx())\n",
    "            pruned_weights = weights * mask\n",
    "            layer.set_weights([pruned_weights])\n",
    "    return pruned_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66ce1989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the original model: 7872194\n",
      "Number of parameters in the pruned model: 7872194\n"
     ]
    }
   ],
   "source": [
    "pruned_model = apply_structured_pruning(model, pruning_percentage=50)\n",
    "original_params = model.count_params()\n",
    "print(\"Number of parameters in the original model:\", original_params)\n",
    "\n",
    "# Count parameters of the pruned model\n",
    "pruned_params = pruned_model.count_params()\n",
    "print(\"Number of parameters in the pruned model:\", pruned_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47d3c196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the original model: 7872194\n",
      "Number of parameters in the pruned model: 7872194\n"
     ]
    }
   ],
   "source": [
    "model.get_layer('runetpp_down0_0').output # Example: Insert StructuredPruning layer after 'conv1' layer\n",
    "\n",
    "pruning_percentage = 50  # 50% pruning\n",
    "\n",
    "# Apply structured pruning to the model\n",
    "pruned_model = apply_structured_pruning(model, pruning_percentage)\n",
    "original_params = model.count_params()\n",
    "print(\"Number of parameters in the original model:\", original_params)\n",
    "\n",
    "# Count parameters of the pruned model\n",
    "pruned_params = pruned_model.count_params()\n",
    "print(\"Number of parameters in the pruned model:\", pruned_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fec5454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pruned_model(original_model, pruning_percentage):\n",
    "    pruned_model = Model(original_model.input, original_model.output)\n",
    "    for layer in original_model.layers:\n",
    "        if isinstance(layer, StructuredPruning):\n",
    "            pruning_mask = layer.get_weights()[0]\n",
    "            pruned_indices = (pruning_mask < (pruning_percentage / 100.0))\n",
    "            if any(pruned_indices):\n",
    "                if hasattr(layer, 'kernel_size'):  # Prune Conv2D layers\n",
    "                    pruned_model.get_layer(layer.name).kernel = tf.boolean_mask(layer.kernel, K.expand_dims(pruned_indices, axis=-1), axis=-2)\n",
    "                    if layer.use_bias:\n",
    "                        pruned_model.get_layer(layer.name).bias = tf.boolean_mask(layer.bias, pruned_indices)\n",
    "                elif hasattr(layer, 'filters'):  # Prune DepthwiseConv2D layers\n",
    "                    pruned_model.get_layer(layer.name).depthwise_kernel = tf.boolean_mask(layer.depthwise_kernel, K.expand_dims(pruned_indices, axis=-1), axis=-2)\n",
    "                    if layer.use_bias:\n",
    "                        pruned_model.get_layer(layer.name).bias = tf.boolean_mask(layer.bias, pruned_indices)\n",
    "    return pruned_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94fde99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the pruned model: 7872194\n"
     ]
    }
   ],
   "source": [
    "pruned_model = create_pruned_model(model, pruning_percentage)\n",
    "\n",
    "# Count parameters of the pruned model\n",
    "pruned_params = pruned_model.count_params()\n",
    "print(\"Number of parameters in the pruned model:\", pruned_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154e495",
   "metadata": {},
   "source": [
    "## Lottery ticket training\n",
    "Lottery Ticket Pruning is a technique in neural network pruning that aims to identify sparse subnetworks (lottery tickets) within the original over-parameterized network, which can achieve comparable performance when trained in isolation. The code for Lottery Ticket Pruning involves iterative training and pruning steps to identify and prune unimportant weights based on their magnitudes, followed by resetting the remaining weights to their initial values.\n",
    "\n",
    "The code for Lottery Ticket Pruning can be summarized as follows:\n",
    "\n",
    "Initialize a neural network with random weights.\n",
    "Iteratively train the network for a fixed number of iterations and prune a certain percentage of the smallest magnitude weights.\n",
    "Reset the remaining weights to their initial values and repeat the process until the desired level of sparsity is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f183870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class LotteryTicketPruning:\n",
    "    def __init__(self, prune_fraction):\n",
    "        self.prune_fraction = prune_fraction\n",
    "\n",
    "    def prune_weights(self, weights):\n",
    "        # Calculate the importance scores of the weights\n",
    "        importance_scores = [np.abs(w) for w in weights]\n",
    "\n",
    "        # Prune a fraction of the lowest importance weights\n",
    "        pruned_weights = []\n",
    "        for i, score in enumerate(importance_scores):\n",
    "            num_params = int(np.prod(score.shape))\n",
    "            num_prune = int(num_params * self.prune_fraction)\n",
    "            if num_prune > 0:\n",
    "                flat_scores = score.flatten()\n",
    "                threshold = np.sort(np.abs(flat_scores))[num_prune]\n",
    "                mask = np.where(np.abs(score) <= threshold, 0, 1)\n",
    "                pruned_weights.append(weights[i] * mask.reshape(score.shape))\n",
    "            else:\n",
    "                pruned_weights.append(weights[i])\n",
    "\n",
    "        return pruned_weights\n",
    "\n",
    "    def prune_model(self, model):\n",
    "        # Get the weights of the model\n",
    "        weights = model.get_weights()\n",
    "\n",
    "        # Prune the weights\n",
    "        pruned_weights = self.prune_weights(weights)\n",
    "\n",
    "        # Create a new model with the pruned weights\n",
    "        pruned_model = self.build_pruned_model(model)\n",
    "        pruned_model.set_weights(pruned_weights)\n",
    "        # Calculate the number of remaining parameters\n",
    "        remaining_params = np.sum([np.sum(np.abs(w) > 0) for w in pruned_weights])\n",
    "        print(\"Remaining Parameters:\", remaining_params)\n",
    "\n",
    "        return pruned_model\n",
    "\n",
    "        return pruned_model\n",
    "\n",
    "    def build_pruned_model(self, model):\n",
    "        # Build a new model with the same architecture as the original model\n",
    "        pruned_model = tf.keras.models.clone_model(model)\n",
    "        pruned_model.build(model.input_shape)\n",
    "\n",
    "        return pruned_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d462a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining Parameters: 32455\n"
     ]
    }
   ],
   "source": [
    "# Specify the prune fraction\n",
    "prune_fraction = 0.2\n",
    "\n",
    "# Create an instance of the LotteryTicketPruning class\n",
    "lottery_ticket_pruning = LotteryTicketPruning(prune_fraction)\n",
    "\n",
    "# Prune the pretrained model\n",
    "pruned_model = lottery_ticket_pruning.prune_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3aedc",
   "metadata": {},
   "source": [
    "## Bayesian Pruning\n",
    "Bayesian Pruning is a technique in neural network pruning that leverages Bayesian approximation to estimate the posterior distribution of weights. It prunes weights based on their uncertainty, considering both weight magnitudes and their corresponding uncertainty estimates. The code for Bayesian Pruning involves calculating weight uncertainties using techniques like Variational Dropout and pruning weights with lower uncertainties.\n",
    "\n",
    "The code for Bayesian Pruning can be summarized as follows:\n",
    "\n",
    "Train a neural network with Variational Dropout enabled to obtain weight uncertainty estimates.\n",
    "Calculate weight uncertainties based on the dropout masks or other Bayesian approximation methods.\n",
    "Prune weights based on a defined criterion, considering both weight magnitudes and uncertainties, to remove less important weights and achieve model compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66950b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class BayesianPruning:\n",
    "    def __init__(self, prune_fraction):\n",
    "        self.prune_fraction = prune_fraction\n",
    "\n",
    "    def prune_weights(self, weights):\n",
    "        # Calculate the magnitudes of the weights\n",
    "        magnitudes = [np.abs(w) for w in weights]\n",
    "\n",
    "        # Prune a fraction of the lowest magnitude weights\n",
    "        pruned_weights = []\n",
    "        for i, magnitude in enumerate(magnitudes):\n",
    "            num_params = int(np.prod(magnitude.shape))\n",
    "            num_prune = int(num_params * self.prune_fraction)\n",
    "            if num_prune > 0:\n",
    "                flat_magnitudes = magnitude.flatten()\n",
    "                threshold = np.sort(flat_magnitudes)[num_prune]\n",
    "                mask = np.where(magnitude <= threshold, 0, 1)\n",
    "                pruned_weights.append(weights[i] * mask.reshape(magnitude.shape))\n",
    "            else:\n",
    "                pruned_weights.append(weights[i])\n",
    "\n",
    "        return pruned_weights\n",
    "\n",
    "    def prune_model(self, model):\n",
    "        # Get the weights of the model\n",
    "        weights = model.get_weights()\n",
    "\n",
    "        # Prune the weights\n",
    "        pruned_weights = self.prune_weights(weights)\n",
    "\n",
    "        # Create a new model with the pruned weights\n",
    "        pruned_model = self.build_pruned_model(model)\n",
    "        pruned_model.set_weights(pruned_weights)\n",
    "\n",
    "        # Calculate the number of remaining parameters\n",
    "        remaining_params = np.sum([np.sum(np.abs(w) > 0) for w in pruned_weights])\n",
    "        print(\"Remaining Parameters:\", remaining_params)\n",
    "\n",
    "        return pruned_model\n",
    "\n",
    "    def build_pruned_model(self, model):\n",
    "        # Build a new model with the same architecture as the original model\n",
    "        pruned_model = tf.keras.models.clone_model(model)\n",
    "        pruned_model.build(model.input_shape)\n",
    "\n",
    "        return pruned_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e9beea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining Parameters: 32455\n"
     ]
    }
   ],
   "source": [
    "prune_fraction = 0.2\n",
    "\n",
    "# Create an instance of the BayesianPruning class\n",
    "bayesian_pruning = BayesianPruning(prune_fraction)\n",
    "\n",
    "# Prune the model based on weight magnitudes\n",
    "pruned_model = bayesian_pruning.prune_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3011e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
